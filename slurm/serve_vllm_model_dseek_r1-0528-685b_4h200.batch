#!/bin/bash
#SBATCH --time=4:00:00
#SBATCH --nodes=1 --ntasks-per-node=1 --cpus-per-task=4
#SBATCH --partition=h200fast --gres=gpu:4
#SBATCH --no-requeue
#SBATCH --mem=64G
#SBATCH --out=../logs/serve.%j.out

# if PROJECT_DIR is not defined, then expect we are in ${PROJECT_DIR}/slurm

export OUTLINES_CACHE_DIR=/tmp/.drchajan.vlll.outlines

export CUDA_VISIBLE_DEVICES=`/home/drchajan/bin/query_empty_gpus,py`
echo CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}

VLLM_MY_PORT=8333
echo "VLLM_MY_PORT=${VLLM_MY_PORT}"
cd /home/drchajan/devel/python/FC
source init_environment_vllm_amd.sh

# export LLM_DIR="cognitivecomputations/DeepSeek-V3-0324-AWQ"
# export LLM_DIR="unsloth/DeepSeek-R1-0528"
export LLM_DIR="RedHatAI/DeepSeek-R1-0528-quantized.w4a16"
#export MAX_MODEL_LEN=32768
# export MAX_MODEL_LEN=65536
export MAX_MODEL_LEN=163840

export NGPUS=4
export VLLM_USE_V1=0
# export VLLM_WORKER_MULTIPROC_METHOD=spawn
# export VLLM_MARLIN_USE_ATOMIC_ADD=1

vllm serve $LLM_DIR --port "${VLLM_MY_PORT}" --gpu-memory-utilization 0.95 --max-model-len ${MAX_MODEL_LEN} --max-seq-len-to-capture ${MAX_MODEL_LEN} --enable-chunked-prefill --enable-prefix-caching --trust-remote-code --tensor-parallel-size ${NGPUS} --dtype auto --enable-reasoning --reasoning-parser deepseek_r1
