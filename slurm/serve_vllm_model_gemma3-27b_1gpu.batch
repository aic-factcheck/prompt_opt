#!/bin/bash
#SBATCH --time=4:00:00
#SBATCH --nodes=1 --ntasks-per-node=1 --cpus-per-task=4
#SBATCH --partition=amdgpufast --gres=gpu:1
#SBATCH --mem=32G
#SBATCH --out=../logs/serve.%j.out

# if PROJECT_DIR is not defined, then expect we are in ${PROJECT_DIR}/slurm

export OUTLINES_CACHE_DIR=/tmp/.drchajan.vlll.outlines

export CUDA_VISIBLE_DEVICES=`/home/drchajan/bin/query_empty_gpus,py`
echo CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}

VLLM_MY_PORT=8333
echo "VLLM_MY_PORT=${VLLM_MY_PORT}"
cd /home/drchajan/devel/python/FC
source init_environment_vllm_amd.sh

# export LLM_DIR="unsloth/gemma-3-27b-it-GGUF"
# export LLM_DIR="google/gemma-3-27b-it-qat-q4_0-gguf"
export LLM_DIR="gaunernst/gemma-3-27b-it-qat-compressed-tensors"
export LLM_CFG_DIR=${LLM_DIR}

#export LLM_CFG_DIR="unsloth/gemma-3-27b-it"
export MAX_MODEL_LEN=65536

export NGPUS=1
#vllm serve $LLM_DIR --port "${VLLM_MY_PORT}" --gpu-memory-utilization 0.95 --max-model-len ${MAX_MODEL_LEN} --tensor-parallel-size ${NGPUS} --dtype auto --enable-reasoning --reasoning-parser deepseek_r1
# vllm serve ${LLM_DIR} --trust-remote-code --hf-config-path ${LLM_CFG_DIR} -tp ${NGPUS} --max-model-len ${MAX_MODEL_LEN} --tokenizer ${LLM_CFG_DIR} --hf-overrides '{"architectures": ["Gemma3ForCausalLM"]}'
vllm serve ${LLM_DIR} --trust-remote-code --hf-config-path ${LLM_CFG_DIR} -tp ${NGPUS} --max-model-len ${MAX_MODEL_LEN} --tokenizer ${LLM_CFG_DIR} --dtype bfloat16
