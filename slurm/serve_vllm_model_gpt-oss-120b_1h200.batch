#!/bin/bash
#SBATCH --time=4:00:00
#SBATCH --nodes=1 --ntasks-per-node=1 --cpus-per-task=4
#SBATCH --partition=h200fast --gres=gpu:1
#SBATCH --no-requeue
#SBATCH --mem=32G
#SBATCH --out=../logs/serve.%j.out

# if PROJECT_DIR is not defined, then expect we are in ${PROJECT_DIR}/slurm

export OUTLINES_CACHE_DIR=/tmp/.drchajan.vlll.outlines

export CUDA_VISIBLE_DEVICES=`/home/drchajan/bin/query_empty_gpus,py`
echo CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}

VLLM_MY_PORT=8333
echo "VLLM_MY_PORT=${VLLM_MY_PORT}"
cd ..
source init_environment_promptopt2_amd.sh

export LLM_DIR="openai/gpt-oss-120b"
export MAX_MODEL_LEN=10240

export NGPUS=1
# see https://github.com/vllm-project/vllm/discussions/15644
#export VLLM_USE_V1=0

#vllm serve $LLM_DIR --port "${VLLM_MY_PORT}" --gpu-memory-utilization 0.9 --max-model-len ${MAX_MODEL_LEN} --tensor-parallel-size ${NGPUS} --dtype auto --enable-prefix-caching
#vllm serve $LLM_DIR --port "${VLLM_MY_PORT}" --gpu-memory-utilization 0.9 --tensor-parallel-size ${NGPUS} --no-enable-prefix-caching --async-scheduling --cuda-graph-sizes 2048 --max-num-batched-tokens 8192 --max-num-seqs 512

vllm serve $LLM_DIR --port "${VLLM_MY_PORT}" --gpu-memory-utilization 0.9 --tensor-parallel-size ${NGPUS} --no-enable-prefix-caching --async-scheduling
