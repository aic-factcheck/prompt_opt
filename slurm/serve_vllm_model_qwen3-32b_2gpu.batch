#!/bin/bash
#SBATCH --time=4:00:00
#SBATCH --nodes=1 --ntasks-per-node=1 --cpus-per-task=4
#SBATCH --partition=amdgpufast --gres=gpu:2
#SBATCH --no-requeue
#SBATCH --mem=32G
#SBATCH --out=../logs/serve.%j.out

# if PROJECT_DIR is not defined, then expect we are in ${PROJECT_DIR}/slurm

export OUTLINES_CACHE_DIR=/tmp/.drchajan.vlll.outlines

export CUDA_VISIBLE_DEVICES=`/home/drchajan/bin/query_empty_gpus,py`
echo CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}

VLLM_MY_PORT=8333
echo "VLLM_MY_PORT=${VLLM_MY_PORT}"
cd /home/drchajan/devel/python/FC
source init_environment_vllm_amd.sh

export LLM_DIR="CobraMamba/Qwen3-32B-AWQ"
export MAX_MODEL_LEN=131072

export NGPUS=2
# see https://github.com/vllm-project/vllm/discussions/15644
export VLLM_USE_V1=0

vllm serve $LLM_DIR --port "${VLLM_MY_PORT}" --gpu-memory-utilization 0.95 --max-model-len ${MAX_MODEL_LEN} --tensor-parallel-size ${NGPUS} --dtype auto --enable-prefix-caching --enable-reasoning --reasoning-parser deepseek_r1 --rope-scaling '{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}'
