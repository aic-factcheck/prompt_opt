import time
import threading
import uuid
import tiktoken

from openai.types.chat import ChatCompletion, ChatCompletionMessage

from loguru import logger

# generated by GPT5

def num_tokens_from_messages(messages, model: str) -> int:
    """
    Compute prompt tokens for a list of chat messages using tiktoken.
    """
    try:
        encoder = tiktoken.encoding_for_model(model)
    except Exception:
        encoder = tiktoken.get_encoding("cl100k_base")

    tokens_per_message = 4  # every message wrapper overhead for GPT-3.5/4-style models
    tokens_per_name = -1
    num_tokens = 0
    for m in messages:
        num_tokens += tokens_per_message
        for k, v in m.items():
            if v is None:
                continue
            num_tokens += len(encoder.encode(v))
            if k == "name":
                num_tokens += tokens_per_name
    num_tokens += 3  # assistant priming
    return num_tokens


def create_completion_streaming(client, model: str, messages, **kwopts) -> ChatCompletion:
    """
    Streaming chat completion using tiktoken for prompt tokens.
    Returns a ChatCompletion with reasoning and function_call preserved.
    """
    kwopts = dict(kwopts)
    kwopts["stream"] = True

    generated_text = ""
    generated_chars = 0
    reasoning_text = ""
    role = None
    finish_reason = None
    function_call = {"name": "", "arguments": ""}

    prompt_tokens = num_tokens_from_messages(messages, model=model)

    lock = threading.Lock()
    stop_event = threading.Event()
    start_time = time.time()

    def _periodic_logger():
        while not stop_event.wait(120):
            with lock:
                elapsed = time.time() - start_time
                chars = generated_chars
                logger.info(f"[stream] elapsed={int(elapsed)}s chars_generated={chars} chars/sec={chars/elapsed:.2f}")

    threading.Thread(target=_periodic_logger, daemon=True).start()

    stream_iter = client.chat.completions.create(model=model, messages=messages, **kwopts)

    try:
        for chunk in stream_iter:
            choice = chunk.choices[0]
            delta = getattr(choice, "delta", None)
            if delta is None:
                if hasattr(choice, "finish_reason") and choice.finish_reason:
                    finish_reason = choice.finish_reason
                continue

            if getattr(delta, "role", None):
                role = delta.role

            if getattr(delta, "content", None):
                with lock:
                    generated_text += delta.content
                    generated_chars += len(delta.content)

            if getattr(delta, "reasoning", None):
                reasoning_text += delta.reasoning

            if getattr(delta, "function_call", None):
                fc = delta.function_call
                if getattr(fc, "name", None):
                    function_call["name"] += fc.name
                if getattr(fc, "arguments", None):
                    function_call["arguments"] += fc.arguments

            if hasattr(choice, "finish_reason") and choice.finish_reason:
                finish_reason = choice.finish_reason

    finally:
        stop_event.set()
        # logger.info("[stream] completed streaming")
        # allow logger thread to exit
        time.sleep(0.1)

    # Build final ChatCompletionMessage
    msg = ChatCompletionMessage(
        role=role or "assistant",
        content=generated_text,
        reasoning=reasoning_text or None,
        function_call=function_call if (function_call["name"] or function_call["arguments"]) else None,
    )

    completion = ChatCompletion(
        id=str(uuid.uuid4()),
        object="chat.completion",
        created=int(time.time()),
        model=model,
        choices=[
            {
                "index": 0,
                "message": msg,
                "finish_reason": finish_reason,
            }
        ],
        usage={
            "prompt_tokens": prompt_tokens,
            "completion_tokens": generated_chars,  # still using chars for completion
            "total_tokens": prompt_tokens + generated_chars,
        },
    )

    return completion
